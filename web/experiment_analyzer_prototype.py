#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿ˆå…‹å°”é€Šå¹²æ¶‰å®éªŒAIåˆ†æåŸå‹
æ•´åˆè§†é¢‘åˆ†æå’Œè®¾å¤‡æ£€æµ‹åŠŸèƒ½ï¼Œç”¨äºæ•™å­¦è¯„ä¼°

åŸºäºç°æœ‰çš„video_test.pyå’Œimagetest_batch.pyä»£ç 
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import json
import time
from datetime import timedelta
from PIL import Image, ImageDraw, ImageFont
from typing import List, Dict, Any, Optional, Tuple

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MichelsonInterferometerAnalyzer:
    """è¿ˆå…‹å°”é€Šå¹²æ¶‰ä»ªå®éªŒåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # é¢„å®šä¹‰çš„æ•™å¸ˆå®éªŒæ­¥éª¤ï¼ˆæ ‡å‡†æµç¨‹ - é€‚åº”1åˆ†55ç§’è§†é¢‘ï¼‰
        self.teacher_steps = [
            {
                "step_id": 1,
                "name": "è¿ˆå…‹å°”é€Šå¹²æ¶‰ä»ªåˆå§‹è®¾ç½®",
                "start_time": 5,
                "duration": 20,
                "key_actions": ["å®‰è£…æ°¦æ°–æ¿€å…‰å™¨", "ç¡®ä¿æ¶é—´éš™å‡åŒ€", "å‡†å¤‡å…‰å­¦å…ƒä»¶"],
                "required_equipment": ["æ°¦æ°–æ¿€å…‰å™¨"],
                "success_criteria": ["æ¿€å…‰å™¨æ­£ç¡®å®‰è£…", "æ¡†æ¶æ°´å¹³ç¨³å®š"]
            },
            {
                "step_id": 2,
                "name": "æ¿€å…‰å™¨å¯¹å‡†å’Œè°ƒèŠ‚",
                "start_time": 25,
                "duration": 20,
                "key_actions": ["è°ƒèŠ‚æ¿€å…‰å™¨ä½ç½®", "è°ƒèŠ‚å…‰æŸé€šè¿‡åˆ†æŸå™¨", "ä½¿å…‰ç‚¹é‡åˆ"],
                "required_equipment": ["æ°¦æ°–æ¿€å…‰å™¨", "åˆ†æŸå™¨å’Œè¡¥å¿æ¿"],
                "success_criteria": ["å…‰æŸå¯¹å‡†", "ä¸¤ä¸ªå…‰ç‚¹é‡åˆ"]
            },
            {
                "step_id": 3,
                "name": "è·å¾—å¹²æ¶‰æ¡çº¹",
                "start_time": 45,
                "duration": 20,
                "key_actions": ["åŠ å…¥æ‰©æŸå™¨", "è°ƒèŠ‚åŠ¨é•œæ‰‹é’®", "è·å¾—å¹²æ¶‰æ¡çº¹"],
                "required_equipment": ["æ‰©æŸå™¨", "åŠ¨é•œ"],
                "success_criteria": ["å‡ºç°æ¸…æ™°å¹²æ¶‰æ¡çº¹", "æ¡çº¹ä½äºä¸­å¿ƒ"]
            },
            {
                "step_id": 4,
                "name": "è§‚å¯Ÿç­‰å€¾å¹²æ¶‰å›¾",
                "start_time": 65,
                "duration": 15,
                "key_actions": ["è½¬åŠ¨ç²¾å¯†æµ‹å¾®å¤´", "è°ƒèŠ‚æµ‹å¾®å¤´", "è§‚å¯Ÿåœ†å½¢å¹²æ¶‰ç¯"],
                "required_equipment": ["ç²¾å¯†æµ‹å¾®å¤´"],
                "success_criteria": ["å‡ºç°åœ†å½¢å¹²æ¶‰ç¯", "ç¯å¿ƒåœ¨å±ä¸­å¤®"]
            },
            {
                "step_id": 5,
                "name": "ç²¾å¯†æµ‹é‡è¿‡ç¨‹",
                "start_time": 80,
                "duration": 20,
                "key_actions": ["è®°å½•æµ‹å¾®å¤´è¯»æ•°", "æ—‹è½¬æµ‹å¾®èºæ—‹", "è®¡æ•°å¹²æ¶‰ç¯å˜åŒ–"],
                "required_equipment": ["ç²¾å¯†æµ‹å¾®å¤´"],
                "success_criteria": ["å‡†ç¡®è®°å½•è¯»æ•°", "æ­£ç¡®è®¡æ•°ç¯æ•°"]
            },
            {
                "step_id": 6,
                "name": "æ³•å¸ƒé‡Œ-ç€ç½—å¹²æ¶‰è®¾ç½®",
                "start_time": 100,
                "duration": 15,
                "key_actions": ["å–ä¸‹åˆ†æŸå™¨å’Œè¡¥å¿æ¿", "å®‰è£…é•€è†œé¢", "è°ƒèŠ‚é•œé¢é—´éš™"],
                "required_equipment": ["å®šé•œ", "åŠ¨é•œ"],
                "success_criteria": ["æ­£ç¡®ç§»é™¤éƒ¨ä»¶", "é•œé¢é—´éš™åˆé€‚"]
            }
        ]
        
        # è®¾å¤‡æ˜ å°„ï¼ˆåŸºäºimagetest_batch.pyï¼‰
        self.equipment_mapping = {
            'helium_neon_laser': 'æ°¦æ°–æ¿€å…‰å™¨',
            'beam_splitter': 'åˆ†æŸå™¨å’Œè¡¥å¿æ¿',
            'moving_mirror': 'åŠ¨é•œ',
            'fixed_mirror': 'å®šé•œ',
            'micrometer_head': 'ç²¾å¯†æµ‹å¾®å¤´',
            'beam_expander': 'æ‰©æŸå™¨',
            'observation_screen': 'äºŒåˆä¸€è§‚å¯Ÿå±'
        }
        
        # å®šä¹‰éƒ¨ä»¶æ–‡ä»¶æ˜ å°„ï¼ˆä¸imagetest_batch.pyä¿æŒä¸€è‡´ï¼‰
        self.component_mapping = {
            'part1.png': {'chinese': 'æ°¦æ°–æ¿€å…‰å™¨', 'english': 'Helium-Neon Laser'},
            'part2.png': {'chinese': 'åˆ†æŸå™¨å’Œè¡¥å¿æ¿', 'english': 'Beam Splitter and Compensator Plate'},
            'part3.png': {'chinese': 'åŠ¨é•œ', 'english': 'Moving Mirror'},
            'part4.png': {'chinese': 'å®šé•œ', 'english': 'Fixed Mirror'},
            'part5.png': {'chinese': 'ç²¾å¯†æµ‹å¾®å¤´', 'english': 'Precision Micrometer Head'},
            'part6.png': {'chinese': 'æ‰©æŸå™¨', 'english': 'Beam Expander'},
            'part7.png': {'chinese': 'äºŒåˆä¸€è§‚å¯Ÿå±', 'english': 'Combination Observation Screen'}
        }
        
        # æ£€æµ‹é¢œè‰²
        self.colors = [
            (0, 0, 255),    # çº¢è‰²
            (0, 255, 0),    # ç»¿è‰²
            (255, 0, 0),    # è“è‰²
            (0, 255, 255),  # é»„è‰²
            (255, 255, 0),  # é’è‰²
            (128, 0, 128),  # ç´«è‰²
            (255, 165, 0)   # æ©™è‰²
        ]

    def extract_key_frames(self, video_path: str, interval: int = 15) -> List[Dict]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        print(f"æ­£åœ¨åˆ†æè§†é¢‘: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        print(f"è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.2f} FPS, æ—¶é•¿: {timedelta(seconds=int(duration))}")
        
        key_frames = []
        timestamps = list(range(0, int(duration), interval))
        
        for timestamp in timestamps:
            frame_number = int(timestamp * fps)
            if frame_number >= total_frames:
                continue
                
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = cap.read()
            
            if ret:
                key_frames.append({
                    'timestamp': timestamp,
                    'frame_number': frame_number,
                    'frame': cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),
                    'analysis': None
                })
        
        cap.release()
        return key_frames

    def draw_chinese_text(self, img, text, position, font_size=24, text_color=(0, 0, 255)):
        """Draw Chinese text on image without encoding issues"""
        # åˆ¤æ–­è¾“å…¥å›¾åƒæ ¼å¼å¹¶è½¬æ¢ä¸ºRGBç”¨äºPIL
        if len(img.shape) == 3:
            # å¦‚æœæ˜¯3é€šé“ï¼Œå‡è®¾æ˜¯RGBæ ¼å¼ï¼ˆæ¥è‡ªæˆ‘ä»¬çš„å¤„ç†æµç¨‹ï¼‰
            img_pil = Image.fromarray(img)
        else:
            # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸ºRGB
            img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_GRAY2RGB))

        # Create a drawing context
        draw = ImageDraw.Draw(img_pil)

        # å°è¯•å¤šç§å­—ä½“è·¯å¾„
        font_paths = [
            "C:/Windows/Fonts/simhei.ttf",  # Windows SimHei
            "C:/Windows/Fonts/msyh.ttf",    # Windows Microsoft YaHei
            "C:/Windows/Fonts/simsun.ttc",  # Windows SimSun
            "/System/Library/Fonts/PingFang.ttc",  # macOS
            "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"  # Linux
        ]
        
        font = None
        for font_path in font_paths:
            try:
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, font_size)
                    break
            except:
                continue
        
        if font is None:
            # Fallback to default font
            try:
                font = ImageFont.load_default()
            except:
                # å¦‚æœè¿é»˜è®¤å­—ä½“éƒ½åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨OpenCVç»˜åˆ¶
                cv2.putText(img, text, position, cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2)
                return img

        # Draw the text
        draw.text(position, text, font=font, fill=text_color)

        # è¿”å›RGBæ ¼å¼ï¼ˆä¿æŒä¸è¾“å…¥ä¸€è‡´ï¼‰
        return np.array(img_pil)

    def extract_template_improved(self, labeled_img):
        """Improved template extraction with better red box detection"""
        print("    æ”¹è¿›çš„æ¨¡æ¿æå–æ–¹æ³•...")
        
        # Convert to different color spaces for better red detection
        hsv = cv2.cvtColor(labeled_img, cv2.COLOR_BGR2HSV)
        
        # Multiple red detection strategies
        # Strategy 1: HSV based
        lower_red1 = np.array([0, 50, 50])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([160, 50, 50])
        upper_red2 = np.array([180, 255, 255])
        
        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        hsv_mask = cv2.bitwise_or(mask1, mask2)
        
        # Strategy 2: BGR based (direct red channel)
        b, g, r = cv2.split(labeled_img)
        # Red is dominant and green/blue are low
        red_dominant = (r > 150).astype(np.uint8)
        green_low = (g < 100).astype(np.uint8)
        blue_low = (b < 100).astype(np.uint8)
        bgr_mask = cv2.bitwise_and(red_dominant, cv2.bitwise_and(green_low, blue_low)) * 255
        
        # Combine masks
        combined_mask = cv2.bitwise_or(hsv_mask, bgr_mask)
        
        # Apply morphological operations to clean up the mask
        kernel = np.ones((3,3), np.uint8)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
        combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
        
        # Find contours
        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Find the best rectangular contour
        best_box = None
        max_score = 0
        
        for contour in contours:
            # Get bounding rectangle
            x, y, w, h = cv2.boundingRect(contour)
            area = w * h
            
            # Filter by size and aspect ratio
            if area > 500 and w > 50 and h > 20:
                # Calculate rectangularity (how close to a rectangle)
                contour_area = cv2.contourArea(contour)
                rectangularity = contour_area / area if area > 0 else 0
                
                # Score based on area and rectangularity
                score = area * rectangularity
                
                if score > max_score:
                    max_score = score
                    best_box = (x, y, x+w, y+h)
        
        if best_box is None:
            # Fallback to manual detection
            h, w = labeled_img.shape[:2]
            best_box = (int(0.11*w), int(0.16*h), int(0.30*w), int(0.22*h))
            print("      ä½¿ç”¨æ‰‹åŠ¨ä¼°è®¡çš„è¾¹ç•Œæ¡†")
        else:
            print(f"      è‡ªåŠ¨æ£€æµ‹åˆ°çº¢è‰²è¾¹ç•Œæ¡†: {best_box}")
        
        x1, y1, x2, y2 = best_box
        template = labeled_img[y1:y2, x1:x2]
        
        return template, best_box

    def multi_scale_template_matching(self, target_img, template, scales=[0.8, 0.9, 1.0, 1.1, 1.2]):
        """Multi-scale template matching for better accuracy"""
        print("    æ‰§è¡Œå¤šå°ºåº¦æ¨¡æ¿åŒ¹é…...")
        
        best_match = None
        best_score = 0
        best_scale = 1.0
        
        template_h, template_w = template.shape[:2]
        
        for scale in scales:
            # Resize template
            new_w = int(template_w * scale)
            new_h = int(template_h * scale)
            
            if new_w <= 0 or new_h <= 0 or new_w > target_img.shape[1] or new_h > target_img.shape[0]:
                continue
                
            scaled_template = cv2.resize(template, (new_w, new_h))
            
            # Template matching with multiple methods
            methods = [cv2.TM_CCOEFF_NORMED, cv2.TM_CCORR_NORMED, cv2.TM_SQDIFF_NORMED]
            
            for method in methods:
                result = cv2.matchTemplate(target_img, scaled_template, method)
                
                if method == cv2.TM_SQDIFF_NORMED:
                    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
                    score = 1 - min_val  # Convert to similarity score
                    loc = min_loc
                else:
                    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
                    score = max_val
                    loc = max_loc
                
                if score > best_score:
                    best_score = score
                    best_match = {
                        'location': loc,
                        'size': (new_w, new_h),
                        'score': score,
                        'scale': scale,
                        'method': method
                    }
        
        return best_match

    def feature_based_matching(self, target_img, template):
        """Feature-based matching using SIFT/ORB as backup"""
        print("    å°è¯•åŸºäºç‰¹å¾ç‚¹çš„åŒ¹é…...")
        
        try:
            # Try SIFT first
            try:
                sift = cv2.SIFT_create()
                kp1, des1 = sift.detectAndCompute(template, None)
                kp2, des2 = sift.detectAndCompute(target_img, None)
                detector_name = "SIFT"
            except:
                # Fallback to ORB
                orb = cv2.ORB_create()
                kp1, des1 = orb.detectAndCompute(template, None)
                kp2, des2 = orb.detectAndCompute(target_img, None)
                detector_name = "ORB"
            
            if des1 is None or des2 is None:
                return None
                
            print(f"      ä½¿ç”¨{detector_name}æ£€æµ‹åˆ°æ¨¡æ¿ç‰¹å¾ç‚¹: {len(kp1)}, ç›®æ ‡å›¾åƒç‰¹å¾ç‚¹: {len(kp2)}")
            
            # Match features
            if detector_name == "SIFT":
                bf = cv2.BFMatcher()
                matches = bf.knnMatch(des1, des2, k=2)
                
                # Apply ratio test
                good_matches = []
                for match_pair in matches:
                    if len(match_pair) == 2:
                        m, n = match_pair
                        if m.distance < 0.75 * n.distance:
                            good_matches.append(m)
            else:
                bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
                matches = bf.match(des1, des2)
                good_matches = sorted(matches, key=lambda x: x.distance)[:50]
            
            print(f"      æ‰¾åˆ°{len(good_matches)}ä¸ªæœ‰æ•ˆåŒ¹é…ç‚¹")
            
            if len(good_matches) >= 4:
                # Extract matched keypoints
                src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
                dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
                
                # Find homography
                M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
                
                if M is not None:
                    # Transform template corners to target image
                    h, w = template.shape[:2]
                    corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)
                    transformed_corners = cv2.perspectiveTransform(corners, M)
                    
                    # Calculate bounding box
                    x_coords = transformed_corners[:, 0, 0]
                    y_coords = transformed_corners[:, 0, 1]
                    
                    x_min, x_max = int(np.min(x_coords)), int(np.max(x_coords))
                    y_min, y_max = int(np.min(y_coords)), int(np.max(y_coords))
                    
                    # Calculate match quality based on inliers
                    inliers = np.sum(mask)
                    quality = inliers / len(good_matches)
                    
                    return {
                        'location': (x_min, y_min),
                        'size': (x_max - x_min, y_max - y_min),
                        'score': quality,
                        'method': f'{detector_name}_HOMOGRAPHY'
                    }
            
            return None
            
        except Exception as e:
            print(f"      ç‰¹å¾åŒ¹é…å¤±è´¥: {e}")
            return None

    def detect_single_component(self, labeled_img_path, target_img, component_name, min_confidence=0.3):
        """Detect a single component in the target image"""
        
        # Load labeled image
        if not os.path.exists(labeled_img_path):
            print(f"    é”™è¯¯: æ ‡æ³¨æ–‡ä»¶ {labeled_img_path} ä¸å­˜åœ¨")
            return None
            
        labeled_img = cv2.imread(labeled_img_path)
        if labeled_img is None:
            print(f"    é”™è¯¯: æ— æ³•åŠ è½½æ ‡æ³¨å›¾ç‰‡ {labeled_img_path}")
            return None

        print(f"    æ ‡æ³¨å›¾ç‰‡å°ºå¯¸: {labeled_img.shape}")

        # Extract template with improved method
        template, template_box = self.extract_template_improved(labeled_img)
        x1, y1, x2, y2 = template_box
        print(f"    æå–çš„æ¨¡æ¿åŒºåŸŸ: ({x1}, {y1}) - ({x2}, {y2})")
        print(f"    æ¨¡æ¿å°ºå¯¸: {template.shape}")

        # Method 1: Multi-scale template matching
        multi_scale_result = self.multi_scale_template_matching(target_img, template)
        
        # Method 2: Feature-based matching
        feature_result = self.feature_based_matching(target_img, template)
        
        # Choose the best result
        best_result = None
        method_used = "æ— "
        
        if multi_scale_result and feature_result:
            if multi_scale_result['score'] > feature_result['score']:
                best_result = multi_scale_result
                method_used = f"å¤šå°ºåº¦æ¨¡æ¿åŒ¹é… (å°ºåº¦: {multi_scale_result['scale']:.1f})"
            else:
                best_result = feature_result
                method_used = "ç‰¹å¾ç‚¹åŒ¹é…"
        elif multi_scale_result:
            best_result = multi_scale_result
            method_used = f"å¤šå°ºåº¦æ¨¡æ¿åŒ¹é… (å°ºåº¦: {multi_scale_result['scale']:.1f})"
        elif feature_result:
            best_result = feature_result
            method_used = "ç‰¹å¾ç‚¹åŒ¹é…"
        else:
            # Fallback to basic template matching
            print("    æ‰€æœ‰é«˜çº§æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡æ¿åŒ¹é…")
            result = cv2.matchTemplate(target_img, template, cv2.TM_CCOEFF_NORMED)
            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
            
            best_result = {
                'location': max_loc,
                'size': (template.shape[1], template.shape[0]),
                'score': max_val
            }
            method_used = "åŸºç¡€æ¨¡æ¿åŒ¹é…"

        # Check if confidence is above threshold
        if best_result['score'] < min_confidence:
            print(f"    æ£€æµ‹ç½®ä¿¡åº¦ {best_result['score']:.3f} ä½äºé˜ˆå€¼ {min_confidence}ï¼Œè®¤ä¸ºæœªæ£€æµ‹åˆ°")
            return None

        # Extract detection information
        top_left = best_result['location']
        w, h = best_result['size']
        bottom_right = (top_left[0] + w, top_left[1] + h)
        score = best_result['score']

        print(f"    æœ€ä½³æ£€æµ‹æ–¹æ³•: {method_used}")
        print(f"    æ£€æµ‹ç½®ä¿¡åº¦: {score:.3f}")
        print(f"    æ£€æµ‹åˆ°çš„ç»„ä»¶ä½ç½®: {top_left} - {bottom_right}")
        print(f"    ç»„ä»¶å¤§å°: {w} x {h} åƒç´ ")

        return {
            'name': component_name,
            'bbox': (top_left[0], top_left[1], bottom_right[0], bottom_right[1]),
            'confidence': score,
            'score': score,  # å…¼å®¹imagetest_batch.pyæ ¼å¼
            'method': method_used,
            'component_name': component_name  # å…¼å®¹imagetest_batch.pyæ ¼å¼
        }

    def detect_equipment_in_frame(self, frame: np.ndarray, min_confidence: float = 0.3) -> List[Dict]:
        """åœ¨å¸§ä¸­æ£€æµ‹å®éªŒè®¾å¤‡ï¼ˆçœŸå®æ£€æµ‹ç‰ˆæœ¬ï¼‰"""
        print("  æ­£åœ¨è¿›è¡Œå®éªŒè®¾å¤‡æ£€æµ‹...")
        
        detections = []
        
        # ç¡®ä¿frameæ˜¯BGRæ ¼å¼ç”¨äºOpenCVå¤„ç†ï¼ˆä¸imagetest_batch.pyä¿æŒä¸€è‡´ï¼‰
        if len(frame.shape) == 3 and frame.shape[2] == 3:
            # å¦‚æœæ˜¯RGBæ ¼å¼ï¼Œè½¬æ¢ä¸ºBGRç”¨äºOpenCV
            target_img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        else:
            target_img = frame.copy()
        
        print(f"ç›®æ ‡å›¾ç‰‡å°ºå¯¸: {target_img.shape}")
        print("="*70)
        
        # éå†æ¯ä¸ªéƒ¨ä»¶æ¨¡æ¿è¿›è¡Œæ£€æµ‹ï¼ˆä¸imagetest_batch.pyä¿æŒä¸€è‡´çš„é¡ºåºå’Œé€»è¾‘ï¼‰
        detected_count = 0
        for i, (part_file, component_info) in enumerate(self.component_mapping.items()):
            print(f"\n[{i+1}/7] æ­£åœ¨æ£€æµ‹: {component_info['chinese']} ({component_info['english']})")
            print(f"ä½¿ç”¨æ ‡æ³¨æ–‡ä»¶: {part_file}")
            
            # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(part_file):
                print(f"  è­¦å‘Š: æ ‡æ³¨æ–‡ä»¶ {part_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            # æ£€æµ‹å•ä¸ªç»„ä»¶ï¼ˆä½¿ç”¨ä¸imagetest_batch.pyç›¸åŒçš„å‚æ•°ï¼‰
            detection = self.detect_single_component(
                part_file, 
                target_img, 
                component_info['chinese'], 
                min_confidence
            )
            
            if detection:
                detected_count += 1
                detections.append(detection)
                print(f"  âœ… æ£€æµ‹æˆåŠŸ!")
            else:
                print(f"  âŒ æœªæ£€æµ‹åˆ°")
        
        # è¾“å‡ºæ±‡æ€»ä¿¡æ¯ï¼ˆä¸imagetest_batch.pyä¿æŒä¸€è‡´ï¼‰
        print("\n" + "="*70)
        print("æ£€æµ‹ç»“æœæ±‡æ€»:")
        print("="*70)
        print(f"æ€»è®¡æ£€æµ‹éƒ¨ä»¶æ•°: {len(self.component_mapping)}")
        print(f"æˆåŠŸæ£€æµ‹éƒ¨ä»¶æ•°: {detected_count}")
        print(f"æ£€æµ‹æˆåŠŸç‡: {detected_count/len(self.component_mapping)*100:.1f}%")
        
        return detections

    def identify_experiment_step(self, frame: np.ndarray, timestamp: int, equipment_detections: List[Dict]) -> Dict:
        """è¯†åˆ«å½“å‰å®éªŒæ­¥éª¤"""
        # åŸºäºæ—¶é—´å’Œæ£€æµ‹åˆ°çš„è®¾å¤‡æ¨æ–­å½“å‰æ­¥éª¤
        current_step = None
        confidence = 0.0
        
        for step in self.teacher_steps:
            if step['start_time'] <= timestamp <= step['start_time'] + step['duration']:
                current_step = step
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…éœ€çš„è®¾å¤‡
                detected_equipment = [det['name'] for det in equipment_detections]
                required_count = len(step['required_equipment'])
                detected_count = len([eq for eq in step['required_equipment'] if eq in detected_equipment])
                
                confidence = detected_count / required_count if required_count > 0 else 0.5
                break
        
        if current_step is None:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ—¶é—´æ®µï¼ŒåŸºäºè®¾å¤‡æ¨æµ‹
            current_step = {
                "step_id": 0,
                "name": "æœªè¯†åˆ«æ­¥éª¤",
                "start_time": timestamp,
                "duration": 0,
                "key_actions": ["æœªçŸ¥æ“ä½œ"],
                "required_equipment": [],
                "success_criteria": []
            }
            confidence = 0.3
        
        return {
            'step': current_step,
            'confidence': confidence,
            'detected_equipment': equipment_detections,
            'timestamp': timestamp
        }

    def analyze_video(self, video_path: str, video_type: str = 'student') -> Dict:
        """åˆ†æè§†é¢‘çš„å®Œæ•´æµç¨‹"""
        print(f"\nå¼€å§‹åˆ†æ{video_type}è§†é¢‘...")
        
        # æå–å…³é”®å¸§
        key_frames = self.extract_key_frames(video_path)
        
        # åˆ†ææ¯ä¸€å¸§
        analysis_results = []
        for i, frame_data in enumerate(key_frames):
            print(f"åˆ†æç¬¬ {i+1}/{len(key_frames)} å¸§ (t={frame_data['timestamp']}s)")
            
            # è®¾å¤‡æ£€æµ‹
            equipment_detections = self.detect_equipment_in_frame(frame_data['frame'], min_confidence=0.25)
            
            # æ­¥éª¤è¯†åˆ«
            step_analysis = self.identify_experiment_step(
                frame_data['frame'], 
                frame_data['timestamp'], 
                equipment_detections
            )
            
            frame_data['analysis'] = step_analysis
            analysis_results.append(frame_data)
        
        return {
            'video_path': video_path,
            'video_type': video_type,
            'total_frames_analyzed': len(analysis_results),
            'key_frames': analysis_results,
            'steps_detected': list(set([frame['analysis']['step']['step_id'] for frame in analysis_results]))
        }

    def compare_student_with_teacher(self, student_analysis: Dict, teacher_analysis: Dict = None) -> Dict:
        """å¯¹æ¯”å­¦ç”Ÿå’Œæ•™å¸ˆçš„å®éªŒæ­¥éª¤"""
        print("\nå¼€å§‹å¯¹æ¯”åˆ†æ...")
        
        student_frames = student_analysis['key_frames']
        comparison_results = []
        issues_found = []
        
        # åˆ†ææ¯ä¸ªå­¦ç”Ÿå¸§
        for frame_data in student_frames:
            student_step = frame_data['analysis']['step']
            timestamp = frame_data['timestamp']
            
            # æ‰¾åˆ°å¯¹åº”çš„æ•™å¸ˆæ­¥éª¤
            expected_step = None
            for teacher_step in self.teacher_steps:
                if teacher_step['start_time'] <= timestamp <= teacher_step['start_time'] + teacher_step['duration']:
                    expected_step = teacher_step
                    break
            
            # å¯¹æ¯”åˆ†æ
            if expected_step is None:
                issue_type = "æ—¶é—´åå·®"
                issue_description = f"æ—¶é—´ç‚¹ {timestamp}s æ²¡æœ‰å¯¹åº”çš„æ ‡å‡†æ­¥éª¤"
                is_correct = False
            elif student_step['step_id'] != expected_step['step_id']:
                issue_type = "æ­¥éª¤é”™è¯¯"
                issue_description = f"åº”è¯¥æ‰§è¡Œ '{expected_step['name']}'ï¼Œä½†æ£€æµ‹åˆ° '{student_step['name']}'"
                is_correct = False
            else:
                issue_type = "æ­£ç¡®"
                issue_description = f"æ­£ç¡®æ‰§è¡Œäº† '{student_step['name']}'"
                is_correct = True
            
            comparison_result = {
                'timestamp': timestamp,
                'frame': frame_data['frame'],
                'student_step': student_step,
                'expected_step': expected_step,
                'is_correct': is_correct,
                'issue_type': issue_type,
                'issue_description': issue_description,
                'confidence': frame_data['analysis']['confidence'],
                'detected_equipment': frame_data['analysis']['detected_equipment']
            }
            
            comparison_results.append(comparison_result)
            
            if not is_correct:
                issues_found.append(comparison_result)
        
        return {
            'total_comparisons': len(comparison_results),
            'correct_steps': len([r for r in comparison_results if r['is_correct']]),
            'incorrect_steps': len([r for r in comparison_results if not r['is_correct']]),
            'accuracy_rate': len([r for r in comparison_results if r['is_correct']]) / len(comparison_results),
            'comparison_details': comparison_results,
            'issues_found': issues_found
        }

    def draw_detections_on_frame(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        result_frame = frame.copy()
        
        for i, detection in enumerate(detections):
            name = detection['name']
            bbox = detection['bbox']
            confidence = detection['confidence']
            
            x1, y1, x2, y2 = bbox
            color = self.colors[i % len(self.colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 3)
            
            # ä½¿ç”¨æ”¹è¿›çš„ä¸­æ–‡æ–‡æœ¬ç»˜åˆ¶
            text_position = (x1, max(30, y1 - 10))
            result_frame = self.draw_chinese_text(
                result_frame,
                name,
                text_position,
                font_size=30,
                text_color=color
            )
            
            # åœ¨å³ä¸‹è§’æ·»åŠ ç½®ä¿¡åº¦ä¿¡æ¯ï¼ˆä½¿ç”¨è‹±æ–‡ï¼Œé¿å…å­—ä½“é—®é¢˜ï¼‰
            confidence_text = f"{confidence:.3f}"
            cv2.putText(result_frame, confidence_text, (x1, y2 + 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            
        return result_frame

    def save_analysis_screenshots(self, comparison_results: Dict, output_dir: str = 'analysis_output') -> None:
        """ä¿å­˜åˆ†ææˆªå›¾"""
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nä¿å­˜åˆ†ææˆªå›¾åˆ°: {output_dir}")
        
        # ä¿å­˜é—®é¢˜æˆªå›¾
        for i, issue in enumerate(comparison_results['issues_found']):
            fig, ax = plt.subplots(1, 1, figsize=(15, 10))
            
            # è·å–æ£€æµ‹ç»“æœå¹¶ç»˜åˆ¶
            detections = issue.get('detected_equipment', [])
            annotated_frame = self.draw_detections_on_frame(issue['frame'], detections)
            
            ax.imshow(annotated_frame)
            ax.set_title(f"é—®é¢˜æˆªå›¾ {i+1}: {issue['issue_type']}\n{issue['issue_description']}", 
                        fontsize=14, pad=20)
            ax.axis('off')
            
            # æ·»åŠ æ—¶é—´æˆ³
            ax.text(0.02, 0.98, f"æ—¶é—´: {issue['timestamp']}s", 
                   transform=ax.transAxes, fontsize=12, 
                   verticalalignment='top', 
                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
            
            # æ·»åŠ æ£€æµ‹ä¿¡æ¯
            if detections:
                detection_info = f"æ£€æµ‹åˆ°è®¾å¤‡: {', '.join([d['name'] for d in detections])}"
                ax.text(0.02, 0.02, detection_info, 
                       transform=ax.transAxes, fontsize=10, 
                       verticalalignment='bottom',
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/issue_{i+1:02d}_t{issue['timestamp']}s.png", 
                       dpi=150, bbox_inches='tight')
            plt.close()
        
        # ä¿å­˜æ­£ç¡®æ­¥éª¤çš„ç¤ºä¾‹æˆªå›¾
        correct_steps = [r for r in comparison_results['comparison_details'] if r['is_correct']]
        for i, correct in enumerate(correct_steps[:3]):  # åªä¿å­˜å‰3ä¸ªæ­£ç¡®ç¤ºä¾‹
            fig, ax = plt.subplots(1, 1, figsize=(15, 10))
            
            # è·å–æ£€æµ‹ç»“æœå¹¶ç»˜åˆ¶
            detections = correct.get('detected_equipment', [])
            annotated_frame = self.draw_detections_on_frame(correct['frame'], detections)
            
            ax.imshow(annotated_frame)
            ax.set_title(f"æ­£ç¡®ç¤ºä¾‹ {i+1}: {correct['issue_description']}", 
                        fontsize=14, pad=20)
            ax.axis('off')
            
            ax.text(0.02, 0.98, f"æ—¶é—´: {correct['timestamp']}s", 
                   transform=ax.transAxes, fontsize=12, 
                   verticalalignment='top', 
                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
            
            # æ·»åŠ æ£€æµ‹ä¿¡æ¯
            if detections:
                detection_info = f"æ£€æµ‹åˆ°è®¾å¤‡: {', '.join([d['name'] for d in detections])}"
                ax.text(0.02, 0.02, detection_info, 
                       transform=ax.transAxes, fontsize=10, 
                       verticalalignment='bottom',
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/correct_{i+1:02d}_t{correct['timestamp']}s.png", 
                       dpi=150, bbox_inches='tight')
            plt.close()

    def generate_analysis_report(self, student_analysis: Dict, comparison_results: Dict, 
                               output_file: str = 'analysis_report.json') -> None:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        
        report = {
            'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'student_video': student_analysis['video_path'],
            'total_frames_analyzed': student_analysis['total_frames_analyzed'],
            'steps_completion': {
                'expected_steps': len(self.teacher_steps),
                'detected_steps': len(student_analysis['steps_detected']),
                'completion_rate': len(student_analysis['steps_detected']) / len(self.teacher_steps)
            },
            'accuracy_assessment': {
                'total_comparisons': comparison_results['total_comparisons'],
                'correct_steps': comparison_results['correct_steps'],
                'incorrect_steps': comparison_results['incorrect_steps'],
                'accuracy_rate': comparison_results['accuracy_rate']
            },
            'issues_summary': [
                {
                    'timestamp': issue['timestamp'],
                    'issue_type': issue['issue_type'],
                    'description': issue['issue_description'],
                    'expected_step': issue['expected_step']['name'] if issue['expected_step'] else 'None',
                    'detected_step': issue['student_step']['name'],
                    'confidence': issue['confidence']
                }
                for issue in comparison_results['issues_found']
            ],
            'recommendations': self.generate_recommendations(comparison_results)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return report

    def generate_recommendations(self, comparison_results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        accuracy = comparison_results['accuracy_rate']
        issues = comparison_results['issues_found']
        
        if accuracy >= 0.8:
            recommendations.append("æ€»ä½“è¡¨ç°è‰¯å¥½ï¼Œå®éªŒæ­¥éª¤åŸºæœ¬æ­£ç¡®")
        elif accuracy >= 0.6:
            recommendations.append("å®éªŒæ­¥éª¤å¤§éƒ¨åˆ†æ­£ç¡®ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        else:
            recommendations.append("å®éªŒæ­¥éª¤å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œå»ºè®®é‡æ–°å­¦ä¹ æ ‡å‡†æµç¨‹")
        
        # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆå…·ä½“å»ºè®®
        issue_types = [issue['issue_type'] for issue in issues]
        
        if "æ­¥éª¤é”™è¯¯" in issue_types:
            recommendations.append("æ³¨æ„æŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ‰§è¡Œå®éªŒæ­¥éª¤")
        
        if "æ—¶é—´åå·®" in issue_types:
            recommendations.append("å»ºè®®æ§åˆ¶å¥½æ¯ä¸ªæ­¥éª¤çš„æ—¶é—´ï¼Œé¿å…è¿‡å¿«æˆ–è¿‡æ…¢")
        
        recommendations.extend([
            "å»ºè®®è§‚çœ‹æ•™å¸ˆç¤ºèŒƒè§†é¢‘ï¼Œæ³¨æ„å…³é”®æ“ä½œç»†èŠ‚",
            "é‡ç‚¹å…³æ³¨è®¾å¤‡çš„æ­£ç¡®ä½¿ç”¨æ–¹æ³•",
            "å®éªŒè¿‡ç¨‹ä¸­è¦ä»”ç»†è§‚å¯Ÿå¹²æ¶‰æ¡çº¹çš„å˜åŒ–"
        ])
        
        return recommendations

    def print_analysis_summary(self, report: Dict) -> None:
        """æ‰“å°åˆ†ææ€»ç»“"""
        print("\n" + "="*60)
        print("è¿ˆå…‹å°”é€Šå¹²æ¶‰å®éªŒAIåˆ†ææŠ¥å‘Š")
        print("="*60)
        
        print(f"åˆ†ææ—¶é—´: {report['analysis_time']}")
        print(f"å­¦ç”Ÿè§†é¢‘: {report['student_video']}")
        print(f"åˆ†æå¸§æ•°: {report['total_frames_analyzed']}")
        
        print(f"\nå®éªŒå®Œæˆåº¦:")
        print(f"  é¢„æœŸæ­¥éª¤æ•°: {report['steps_completion']['expected_steps']}")
        print(f"  æ£€æµ‹æ­¥éª¤æ•°: {report['steps_completion']['detected_steps']}")
        print(f"  å®Œæˆç‡: {report['steps_completion']['completion_rate']:.1%}")
        
        print(f"\nå‡†ç¡®æ€§è¯„ä¼°:")
        print(f"  æ€»å¯¹æ¯”æ¬¡æ•°: {report['accuracy_assessment']['total_comparisons']}")
        print(f"  æ­£ç¡®æ­¥éª¤: {report['accuracy_assessment']['correct_steps']}")
        print(f"  é”™è¯¯æ­¥éª¤: {report['accuracy_assessment']['incorrect_steps']}")
        print(f"  å‡†ç¡®ç‡: {report['accuracy_assessment']['accuracy_rate']:.1%}")
        
        if report['issues_summary']:
            print(f"\nå‘ç°çš„é—®é¢˜:")
            for i, issue in enumerate(report['issues_summary'], 1):
                print(f"  {i}. [t={issue['timestamp']}s] {issue['issue_type']}: {issue['description']}")
        
        print(f"\næ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"  {i}. {rec}")

def extract_frame_at_time(video_path: str, time_seconds: float = 113.0, output_path: str = 'Identify_target.png'):
    """æå–è§†é¢‘æŒ‡å®šæ—¶é—´ç‚¹çš„å¸§ä½œä¸ºç›®æ ‡å›¾ç‰‡"""
    print(f"æ­£åœ¨æå– {video_path} åœ¨ {time_seconds}ç§’ çš„å¸§...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    
    # è·å–è§†é¢‘ä¿¡æ¯
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps
    
    print(f"è§†é¢‘ä¿¡æ¯: {total_frames} å¸§, {fps:.2f} FPS, æ—¶é•¿: {timedelta(seconds=int(duration))}")
    
    # æ£€æŸ¥æ—¶é—´ç‚¹æ˜¯å¦æœ‰æ•ˆ
    if time_seconds > duration:
        print(f"âš ï¸  æŒ‡å®šæ—¶é—´ {time_seconds}ç§’ è¶…è¿‡è§†é¢‘æ€»æ—¶é•¿ {duration:.1f}ç§’ï¼Œå°†æå–æœ€åä¸€å¸§")
        target_frame_number = total_frames - 1
        time_seconds = duration
    else:
        # è®¡ç®—ç›®æ ‡å¸§å·
        target_frame_number = int(time_seconds * fps)
    
    print(f"ç›®æ ‡æ—¶é—´: {time_seconds}ç§’ (ç¬¬ {target_frame_number} å¸§)")
    
    # è·³åˆ°æŒ‡å®šå¸§
    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame_number)
    ret, frame = cap.read()
    
    if ret:
        # ä¿å­˜æŒ‡å®šæ—¶é—´çš„å¸§
        cv2.imwrite(output_path, frame)
        print(f"âœ… {time_seconds}ç§’çš„å¸§å·²ä¿å­˜ä¸º: {output_path}")
        print(f"å›¾ç‰‡å°ºå¯¸: {frame.shape}")
        cap.release()
        return frame
    else:
        cap.release()
        raise ValueError(f"æ— æ³•è¯»å–è§†é¢‘åœ¨ {time_seconds}ç§’ çš„å¸§")

def analyze_single_frame_detection():
    """åªåˆ†æå•å¸§çš„è®¾å¤‡æ£€æµ‹åŠŸèƒ½"""
    print("è¿ˆå…‹å°”é€Šå¹²æ¶‰å®éªŒè®¾å¤‡æ£€æµ‹ç³»ç»Ÿ")
    print("="*60)
    
    # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶
    required_files = {
        'student.mp4': 'å­¦ç”Ÿå®éªŒè§†é¢‘',
        'part1.png': 'æ°¦æ°–æ¿€å…‰å™¨æ ‡æ³¨å›¾ç‰‡',
        'part2.png': 'åˆ†æŸå™¨å’Œè¡¥å¿æ¿æ ‡æ³¨å›¾ç‰‡',
        'part3.png': 'åŠ¨é•œæ ‡æ³¨å›¾ç‰‡',
        'part4.png': 'å®šé•œæ ‡æ³¨å›¾ç‰‡',
        'part5.png': 'ç²¾å¯†æµ‹å¾®å¤´æ ‡æ³¨å›¾ç‰‡',
        'part6.png': 'æ‰©æŸå™¨æ ‡æ³¨å›¾ç‰‡'
    }
    
    missing_files = []
    for file_path, description in required_files.items():
        if not os.path.exists(file_path):
            missing_files.append(f"  âŒ {file_path} - {description}")
        else:
            print(f"  âœ… {file_path} - {description}")
    
    if missing_files:
        print(f"\nç¼ºå°‘ä»¥ä¸‹å¿…éœ€æ–‡ä»¶:")
        for missing in missing_files:
            print(missing)
        print(f"\nè¯·ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ web/ ç›®å½•ä¸­ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åºã€‚")
        return False
    
    # å¯é€‰æ–‡ä»¶æ£€æŸ¥
    if os.path.exists('part7.png'):
        print(f"  âœ… part7.png - äºŒåˆä¸€è§‚å¯Ÿå±æ ‡æ³¨å›¾ç‰‡")
    else:
        print(f"  âšª part7.png - äºŒåˆä¸€è§‚å¯Ÿå±æ ‡æ³¨å›¾ç‰‡ (å¯é€‰)")
    
    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = MichelsonInterferometerAnalyzer()
        
        # æ­¥éª¤1: æå–æŒ‡å®šæ—¶é—´çš„å¸§ä½œä¸ºç›®æ ‡å›¾ç‰‡  
        print(f"\n{'='*60}")
        print("æ­¥éª¤ 1: æå–student.mp4åœ¨1åˆ†48ç§’çš„å¸§ä½œä¸ºIdentify_target.png")
        print("="*60)
        
        target_frame = extract_frame_at_time('student.mp4', time_seconds=108.0, output_path='Identify_target.png')
        
        # æ­¥éª¤2: å¯¹1åˆ†48ç§’çš„å¸§è¿›è¡Œè®¾å¤‡æ£€æµ‹
        print(f"\n{'='*60}")
        print("æ­¥éª¤ 2: å¯¹1åˆ†48ç§’çš„å¸§è¿›è¡Œè®¾å¤‡æ£€æµ‹")
        print("="*60)
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼ç”¨äºåˆ†æ
        target_frame_rgb = cv2.cvtColor(target_frame, cv2.COLOR_BGR2RGB)
        
        # æ‰§è¡Œè®¾å¤‡æ£€æµ‹
        equipment_detections = analyzer.detect_equipment_in_frame(target_frame_rgb, min_confidence=0.25)
        
        # æ­¥éª¤3: ä¿å­˜æ£€æµ‹ç»“æœå›¾ç‰‡
        print(f"\n{'='*60}")
        print("æ­¥éª¤ 3: ä¿å­˜æ£€æµ‹ç»“æœ")
        print("="*60)
        
        if equipment_detections:
            # åœ¨åŸå›¾ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
            annotated_frame = analyzer.draw_detections_on_frame(target_frame_rgb, equipment_detections)
            
            # ä¿å­˜æ ‡æ³¨åçš„å›¾ç‰‡
            annotated_bgr = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)
            cv2.imwrite('detection_result.png', annotated_bgr)
            
            print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜:")
            print(f"  ğŸ“¸ 1åˆ†48ç§’çš„å¸§: Identify_target.png")
            print(f"  ğŸ“¸ æ ‡æ³¨æ£€æµ‹ç»“æœ: detection_result.png")
            
            # ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š
            print(f"\n{'='*60}")
            print("æ£€æµ‹ç»“æœæŠ¥å‘Š")
            print("="*60)
            
            print(f"æ£€æµ‹åˆ°çš„è®¾å¤‡æ•°é‡: {len(equipment_detections)}")
            print(f"æ£€æµ‹æˆåŠŸç‡: {len(equipment_detections)}/7 = {len(equipment_detections)/7*100:.1f}%")
            
            print(f"\nè¯¦ç»†æ£€æµ‹ç»“æœ:")
            for i, detection in enumerate(equipment_detections, 1):
                name = detection['name']
                confidence = detection['confidence']
                bbox = detection['bbox']
                method = detection['method']
                x1, y1, x2, y2 = bbox
                
                print(f"  {i}. {name}")
                print(f"     ç½®ä¿¡åº¦: {confidence:.3f}")
                print(f"     ä½ç½®: ({x1}, {y1}) - ({x2}, {y2})")
                print(f"     å¤§å°: {x2-x1} x {y2-y1} åƒç´ ")
                print(f"     æ–¹æ³•: {method}")
                print()
            
            # ä¿å­˜JSONæŠ¥å‘Š
            detection_report = {
                'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_video': 'student.mp4',  
                'target_image': 'Identify_target.png',
                'total_components_to_detect': 7,
                'components_detected': len(equipment_detections),
                'detection_rate': len(equipment_detections) / 7,
                'detections': [
                    {
                        'name': det['name'],
                        'confidence': det['confidence'],
                        'bbox': det['bbox'],
                        'method': det['method']
                    }
                    for det in equipment_detections
                ]
            }
            
            with open('detection_report.json', 'w', encoding='utf-8') as f:
                json.dump(detection_report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: detection_report.json")
            
        else:
            print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•è®¾å¤‡")
        
        print(f"\nğŸ‰ å•å¸§è®¾å¤‡æ£€æµ‹å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œè®¾å¤‡æ£€æµ‹åˆ†æ"""
    print("è¿ˆå…‹å°”é€Šå¹²æ¶‰å®éªŒè®¾å¤‡æ£€æµ‹ç³»ç»Ÿ")
    print("="*60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…éœ€æ–‡ä»¶
    if os.path.exists('student.mp4') and any(os.path.exists(f'part{i}.png') for i in range(1, 7)):
        print("æ£€æµ‹åˆ°å­¦ç”Ÿè§†é¢‘å’Œæ ‡æ³¨æ–‡ä»¶ï¼Œå¯åŠ¨è®¾å¤‡æ£€æµ‹æ¨¡å¼...")
        success = analyze_single_frame_detection()
        if success:
            print("\nğŸ‰ è®¾å¤‡æ£€æµ‹å®Œæˆï¼")
        else:
            print("\nğŸ’¡ å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®Œæ•´")
    else:
        print("æœªæ£€æµ‹åˆ°å¿…éœ€çš„æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print("  - student.mp4 (å­¦ç”Ÿå®éªŒè§†é¢‘)")
        print("  - part1.png åˆ° part6.png (è®¾å¤‡æ ‡æ³¨å›¾ç‰‡)")
        print("\nğŸ’¡ ç¨‹åºå°†:")
        print("  1. æå–student.mp4åœ¨1åˆ†48ç§’çš„å¸§ä½œä¸ºIdentify_target.png")
        print("  2. ä½¿ç”¨part1-6.pngæ£€æµ‹1åˆ†48ç§’å¸§ä¸­çš„å®éªŒè®¾å¤‡")
        print("  3. ç”Ÿæˆå¸¦æ ‡æ³¨çš„æ£€æµ‹ç»“æœå›¾ç‰‡")
        print("  4. è¾“å‡ºè¯¦ç»†çš„æ£€æµ‹æŠ¥å‘Š")

if __name__ == "__main__":
    main()